# transformers.py
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler




class InfinityReplacer(BaseEstimator, TransformerMixin):
    """Zamienia inf/-inf na NaN."""
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = X.copy()
        return X.replace([np.inf, -np.inf], np.nan)


class HighMissingDropper(BaseEstimator, TransformerMixin):
    """Usuwa kolumny z liczbƒÖ brak√≥w przekraczajƒÖcƒÖ threshold."""
    
    def __init__(self, missing_threshold=0.95):
        self.missing_threshold = missing_threshold
    
    def fit(self, X, y=None):
        missing_ratio = X.isnull().mean()
        self.cols_to_drop_ = missing_ratio[missing_ratio > self.missing_threshold].index.tolist()
        if len(self.cols_to_drop_) > 0:
            print(f"üóëÔ∏è Zapamiƒôtano {len(self.cols_to_drop_)} kolumn do usuniƒôcia (braki > {self.missing_threshold*100:.0f}%)")
        return self
    
    def transform(self, X):
        X = X.copy()
        return X.drop(columns=self.cols_to_drop_, errors='ignore')


class MissingIndicator(BaseEstimator, TransformerMixin):
    """Dodaje flagi *_missing dla kolumn z brakami."""
    
    def fit(self, X, y=None):
        self.cols_with_missing_ = X.columns[X.isnull().any()].tolist()
        return self
    
    def transform(self, X):
        X = X.copy()
        for col in self.cols_with_missing_:
            if col in X.columns:
                X[f"{col}_missing"] = X[col].isnull().astype(int)
        return X


class CustomImputer(BaseEstimator, TransformerMixin):
    """Imputacja: numeryczne -> mediana, kategoryczne -> moda."""
    
    def __init__(self):
        self.imputer_num_ = None
        self.imputer_cat_ = None
        self.num_cols_ = None
        self.cat_cols_ = None
    
    def fit(self, X, y=None):
        self.num_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()
        bool_cols = X.select_dtypes(include=[bool]).columns.tolist()
        self.num_cols_ = [col for col in self.num_cols_ if col not in bool_cols]
        
        self.cat_cols_ = X.select_dtypes(exclude=[np.number, np.bool_]).columns.tolist()
        
        if len(self.num_cols_) > 0:
            self.imputer_num_ = SimpleImputer(strategy="median")
            self.imputer_num_.fit(X[self.num_cols_])
        
        if len(self.cat_cols_) > 0:
            self.imputer_cat_ = SimpleImputer(strategy="most_frequent")
            self.imputer_cat_.fit(X[self.cat_cols_])
        
        return self
    
    def transform(self, X):
        X = X.copy()
        
        if self.imputer_num_ is not None and len(self.num_cols_) > 0:
            X[self.num_cols_] = self.imputer_num_.transform(X[self.num_cols_])
        
        if self.imputer_cat_ is not None and len(self.cat_cols_) > 0:
            X[self.cat_cols_] = self.imputer_cat_.transform(X[self.cat_cols_])
        
        return X


class Winsorizer(BaseEstimator, TransformerMixin):
    """Winsoryzacja (obcina warto≈õci skrajne na podstawie kwantyli)."""
    
    def __init__(self, lower_q=0.02, upper_q=0.98):
        self.lower_q = lower_q
        self.upper_q = upper_q
    
    def fit(self, X, y=None):
        num_cols = X.select_dtypes(include=[np.number]).columns
        bool_cols = X.select_dtypes(include=[bool]).columns
        num_cols = [col for col in num_cols 
                    if col not in bool_cols and not col.endswith("_missing")]
        
        self.limits_ = {}
        for col in num_cols:
            lower = X[col].quantile(self.lower_q)
            upper = X[col].quantile(self.upper_q)
            self.limits_[col] = (lower, upper)
        
        return self
    
    def transform(self, X):
        X = X.copy()
        for col, (lower, upper) in self.limits_.items():
            if col in X.columns:
                X[col] = np.clip(X[col], lower, upper)
        return X


class NumericScaler(BaseEstimator, TransformerMixin):
    """Standaryzacja kolumn numerycznych (pomija bool i *_missing)."""
    
    def __init__(self):
        self.scaler_ = None
        self.num_cols_ = None
    
    def fit(self, X, y=None):
        num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        bool_cols = X.select_dtypes(include=[bool]).columns.tolist()
        self.num_cols_ = [col for col in num_cols 
                         if col not in bool_cols and not col.endswith("_missing")]
        
        if len(self.num_cols_) > 0:
            self.scaler_ = StandardScaler()
            self.scaler_.fit(X[self.num_cols_])
        return self
    
    def transform(self, X):
        X = X.copy()
        if self.scaler_ is not None and len(self.num_cols_) > 0:
            X[self.num_cols_] = self.scaler_.transform(X[self.num_cols_])
        return X


class OneHotEncoder(BaseEstimator, TransformerMixin):
    """One-hot encoding dla kolumn kategorycznych."""
    
    def __init__(self):
        self.cat_cols_ = None
        self.encoded_cols_ = None
    
    def fit(self, X, y=None):
        self.cat_cols_ = X.select_dtypes(exclude=[np.number]).columns.tolist()
        if len(self.cat_cols_) > 0:
            X_encoded = pd.get_dummies(X, columns=self.cat_cols_, prefix=self.cat_cols_)
            self.encoded_cols_ = X_encoded.columns.tolist()
        else:
            self.encoded_cols_ = X.columns.tolist()
        return self
    
    def transform(self, X):
        X = X.copy()
        if len(self.cat_cols_) > 0:
            X = pd.get_dummies(X, columns=self.cat_cols_, prefix=self.cat_cols_)
            for col in self.encoded_cols_:
                if col not in X.columns:
                    X[col] = 0
            X = X[self.encoded_cols_]
        return X


class LowVarianceDropper(BaseEstimator, TransformerMixin):
    """Usuwa kolumny o niskiej wariancji."""
    
    def __init__(self, var_threshold=0.01):
        self.var_threshold = var_threshold
    
    def fit(self, X, y=None):
        num_cols = X.select_dtypes(include=[np.number, np.bool_]).columns
        variances = X[num_cols].var(numeric_only=True)
        self.low_var_cols_ = variances[variances < self.var_threshold].index.tolist()
        if len(self.low_var_cols_) > 0:
            print(f"‚ö†Ô∏è Zapamiƒôtano {len(self.low_var_cols_)} kolumn o niskiej wariancji (< {self.var_threshold})")
        return self
    
    def transform(self, X):
        return X.drop(columns=self.low_var_cols_, errors='ignore')


class HighCorrelationDropper(BaseEstimator, TransformerMixin):
    """Usuwa kolumny silnie skorelowane."""
    
    def __init__(self, corr_threshold=0.9):
        self.corr_threshold = corr_threshold

    
    def fit(self, X, y=None):
        num_cols = X.select_dtypes(include=[np.number, np.bool_]).columns
        corr_matrix = X[num_cols].corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        self.high_corr_cols_ = [col for col in upper.columns if any(upper[col] > self.corr_threshold)]
        if len(self.high_corr_cols_) > 0:
            print(f"üîÅ Zapamiƒôtano {len(self.high_corr_cols_)} kolumn z wysokƒÖ korelacjƒÖ (> {self.corr_threshold})")
        return self
    
    def transform(self, X):
        return X.drop(columns=self.high_corr_cols_, errors='ignore')




class WoETransformer(BaseEstimator, TransformerMixin):
    """
    Transformer zamieniajƒÖcy zmienne numeryczne na WoE wzglƒôdem y (default flag).
    
    Za≈Ço≈ºenia:
    - y = 1 -> 'bad' (default)
    - y = 0 -> 'good' (brak defaultu)
    
    Dzia≈Ça w dw√≥ch krokach:
    1) dzieli ka≈ºdƒÖ kolumnƒô na n_bins kwantylowych przedzia≈Ç√≥w (+ osobny bin na missing),
    2) liczy WoE dla ka≈ºdego binu i zapisuje s≈Çowniki mapowa≈Ñ.
    """

    def __init__(self, n_bins=5, eps=0.5):
        """
        n_bins: liczba bin√≥w kwantylowych (bez binu na brak)
        eps: smoothing dodawany do licznik√≥w good/bad, ≈ºeby uniknƒÖƒá WoE = +/- inf
        """
        self.n_bins = n_bins
        self.eps = eps

    def fit(self, X, y):
        X = X.copy()
        y = pd.Series(y)

        # bierzemy tylko kolumny numeryczne (WoE ma sens g≈Ç√≥wnie tam)
        self.num_cols_ = X.select_dtypes(include=[np.number]).columns.tolist()

        # globalne liczebno≈õci
        self.total_good_ = (y == 0).sum()
        self.total_bad_ = (y == 1).sum()

        self.bin_edges_ = {}
        self.woe_maps_ = {}
        self.iv_ = {}

        for col in self.num_cols_:
            col_data = X[col]
            df_tmp = pd.DataFrame({"x": col_data, "y": y})

            # osobny bin na braki
            missing_mask = df_tmp["x"].isna()

            # kwantylowy binning na nie-missing
            if (~missing_mask).sum() == 0:
                # kolumna w ca≈Ço≈õci pusta -> WoE = 0
                self.bin_edges_[col] = None
                self.woe_maps_[col] = {"MISSING": 0.0}
                self.iv_[col] = 0.0
                continue

            try:
                # retbins=True -> dostajemy krawƒôdzie przedzia≈Ç√≥w
                _, bins = pd.qcut(
                    df_tmp.loc[~missing_mask, "x"],
                    q=self.n_bins,
                    duplicates="drop",
                    retbins=True
                )
            except ValueError:
                # za ma≈Ço unikalnych warto≈õci -> jeden bin
                bins = np.unique(df_tmp.loc[~missing_mask, "x"])
                if bins.size == 1:
                    bins = np.array([bins[0] - 1e-6, bins[0] + 1e-6])

            self.bin_edges_[col] = bins

            # przypisanie bin√≥w
            df_tmp["bin"] = pd.cut(
                df_tmp["x"],
                bins=bins,
                include_lowest=True
            )
            df_tmp["bin"] = df_tmp["bin"].astype(object)

            df_tmp.loc[missing_mask, "bin"] = "MISSING"

            # agregacja good/bad per bin
            grouped = df_tmp.groupby("bin")["y"]
            good = (grouped.apply(lambda s: (s == 0).sum()) + self.eps)
            bad = (grouped.apply(lambda s: (s == 1).sum()) + self.eps)

            dist_good = good / (self.total_good_ + self.eps * len(good))
            dist_bad = bad / (self.total_bad_ + self.eps * len(bad))

            woe = np.log(dist_good / dist_bad)

            # zapisujemy mapowanie: bin -> WoE
            woe_map = woe.to_dict()
            self.woe_maps_[col] = woe_map

            # policz IV tej zmiennej (przyda siƒô p√≥≈∫niej do raportu)
            iv_col = ((dist_good - dist_bad) * woe).sum()
            self.iv_[col] = iv_col

        return self

    def transform(self, X):
        X = X.copy()

        for col in self.num_cols_:
            if col not in X.columns:
                continue

            col_data = X[col]
            bins = self.bin_edges_[col]
            woe_map = self.woe_maps_[col]

            if bins is not None:
                binned = pd.cut(
                    col_data,
                    bins=bins,
                    include_lowest=True
                ).astype(object)
            else:
                # kolumna by≈Ça w ca≈Ço≈õci missing przy fit
                binned = pd.Series(["MISSING"] * len(X), index=X.index, dtype=object)

            # missing -> "MISSING"
            binned[col_data.isna()] = "MISSING"

            # zamiana bin√≥w na WoE; nieznane biny -> 0.0
            X[col] = binned.map(woe_map).fillna(0.0).astype(float)

        return X


# Dodajemy to bo na pocatku logit wyszedl nie interpretowalny bo duzo wspolczynnikow
# beta bylo dodatnie 

class WoEDirectionalityFilter(BaseEstimator, TransformerMixin):
    """
    Dla cech po WoE:
    - liczy korelacjƒô (domy≈õlnie Spearmana) z targetem
    - zostawia tylko te kolumny, dla kt√≥rych korelacja jest wyra≈∫nie ujemna.
      (czyli: wiƒôksze WoE => mniej default√≥w)
    """

    def __init__(self, min_corr=-0.01, method="spearman", verbose=True):
        """
        min_corr : float
            pr√≥g ujemnej korelacji ‚Äì zostawiamy tylko kolumny z corr < min_corr
            np. -0.01 znaczy: zachowaj, je≈õli korelacja <= -0.01
        method : {"spearman", "pearson"}
        verbose : bool
        """
        self.min_corr = min_corr
        self.method = method
        self.verbose = verbose

    def fit(self, X, y):
        # zadbajmy o DataFrame z nazwami kolumn
        if isinstance(X, pd.DataFrame):
            X_df = X.copy()
        else:
            X_df = pd.DataFrame(X, columns=[f"x_{i}" for i in range(X.shape[1])])

        y_series = pd.Series(y)

        self.corrs_ = {}
        for col in X_df.columns:
            try:
                c = X_df[col].corr(y_series, method=self.method)
            except Exception:
                c = np.nan
            self.corrs_[col] = c

        # zostawiamy kolumny z wyra≈∫nie ujemnƒÖ korelacjƒÖ
        self.cols_to_keep_ = [
            col for col, c in self.corrs_.items()
            if pd.notna(c) and c < self.min_corr
        ]

        if self.verbose:
            total = X_df.shape[1]
            kept = len(self.cols_to_keep_)
            dropped = total - kept
            print(
                f"üßπ WoEDirectionalityFilter: zachowano {kept}/{total} kolumn, "
                f"usuniƒôto {dropped} (corr >= {self.min_corr:.3f})"
            )

        return self

    def transform(self, X):
        if isinstance(X, pd.DataFrame):
            X_df = X
        else:
            # je≈õli X jest macierzƒÖ ‚Äì zak≈Çadamy tƒô samƒÖ kolejno≈õƒá kolumn co w fit
            X_df = pd.DataFrame(X, columns=list(self.corrs_.keys()))

        return X_df[self.cols_to_keep_]


class DropColumnsTransformer(BaseEstimator, TransformerMixin):
    """
    Transformer usuwajƒÖcy wskazane kolumny.

    Mo≈ºna:
    - przekazaƒá listƒô kolumn w parametrze `columns`
    - albo ≈õcie≈ºkƒô do pliku CSV z listƒÖ cech (`columns_path`),
      gdzie kolumna z nazwami cech nazywa siƒô np. 'feature'.

    U≈ºywamy go przed WoE, ≈ºeby wyrzuciƒá cechy z dodatnimi beta / wysokim VIF.
    """

    def __init__(self, columns=None, columns_path=None, feature_col="feature"):
        self.columns = columns
        self.columns_path = columns_path
        self.feature_col = feature_col
        self.columns_ = None

    def fit(self, X, y=None):
        # Je≈õli kolumny podane "na sztywno"
        if self.columns is not None:
            self.columns_ = list(self.columns)
            return self

        # Je≈õli mamy ≈õcie≈ºkƒô do CSV z listƒÖ cech
        if self.columns_path is not None:
            try:
                df_cols = pd.read_csv(self.columns_path)
                if self.feature_col not in df_cols.columns:
                    raise ValueError(
                        f"Plik {self.columns_path} nie zawiera kolumny '{self.feature_col}' "
                        "z nazwami cech."
                    )
                self.columns_ = df_cols[self.feature_col].astype(str).tolist()
                if len(self.columns_) > 0:
                    print(
                        f"üßπ DropColumnsTransformer: zapamiƒôtano {len(self.columns_)} kolumn "
                        f"do usuniƒôcia z pliku {self.columns_path}"
                    )
                else:
                    print(
                        f"üßπ DropColumnsTransformer: plik {self.columns_path} jest pusty ‚Äì "
                        "nie usuwamy ≈ºadnych kolumn."
                    )
            except FileNotFoundError:
                print(
                    f"‚ö†Ô∏è DropColumnsTransformer: nie znaleziono pliku {self.columns_path}. "
                    "Nie usuwamy ≈ºadnych kolumn."
                )
                self.columns_ = []
        else:
            # Nic nie podano ‚Äì transformer jest no-op
            self.columns_ = []

        return self

    def transform(self, X):
        X = X.copy()
        if not self.columns_:
            return X
        return X.drop(columns=self.columns_, errors="ignore")


















